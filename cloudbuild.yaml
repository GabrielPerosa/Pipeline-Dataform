steps:
  # Passo 1: Clonar o repositório
  - name: 'gcr.io/cloud-builders/git'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        git clone https://github.com/GabrielPerosa/Pipeline-Dataform.git
        cd Pipeline-Dataform
        git branch -a  # Verifica todas as branches do repositório
        git fetch origin  # Faz o fetch para garantir que todas as branches sejam baixadas
        git checkout main  # Tenta fazer o checkout na branch main

  # Passo 2: Instalar dependências do SQLFluff
  - name: 'python:3.9'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        pip install --no-cache-dir sqlfluff

  # Passo 3: Verificar qualidade dos scripts SQL
  - name: 'python:3.9'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        # Identificar o email do autor do commit
        COMMIT_AUTHOR_EMAIL="${_AUTHOR_EMAIL}"  # Substituição correta
        # Executar o lint
        sqlfluff lint definitions/ > lint-results.txt
        # Verificar o resultado do lint
        if grep -q "L:  " lint-results.txt; then
          echo "Testes SQL falharam. Notificando erro por email."
          echo "SQL Quality Check Failed. Review the errors in lint-results.txt." | mail -s "SQL Lint Failure" $COMMIT_AUTHOR_EMAIL
          exit 1
        else
          echo "SQL Quality Check Passed."
        fi

  # Passo 4: Mesclar na branch main caso os testes passem
  - name: 'gcr.io/cloud-builders/git'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        git checkout main
        git merge workspace-dataform-teste
        git push origin main

  # Passo 5: Instalar Dataform e executar
  - name: 'gcr.io/cloud-builders/npm'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        cd Pipeline-Dataform  # Certifique-se de estar no diretório correto
        npm install -g @dataform/cli  # Instalar o Dataform globalmente
        dataform run  # Executar as transformações de dados

substitutions:
  _AUTHOR_EMAIL: 'gabrielperosa493@gmail.com'

timeout: '1200s'

options:
  logging: CLOUD_LOGGING_ONLY
